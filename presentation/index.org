# -*- org-reveal-title-slide: "<h1>%t</h1><h2>%a</h2><h4>%d</h4><h4>EMNLP17 (Workshop on Stylistic Variation) - Copenhagen</h4><p><a href=\"https://emanjavacas.github.com/slides-content/copenhagen-emnlp-17\">https://emanjavacas.github.com/slides-content/copenhagen-emnlp-17</a></p>"; -*-

#+TITLE: Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution
#+AUTHOR: Enrique Manjavacas & Jeroen De Gussem & Walter Daelemans & Mike Kestemont
#+DATE: 08/09/2017
#+REVEAL_ROOT: ../externals/reveal.js/
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:t reveal_control:t
#+OPTIONS: reveal_keyboard:t reveal_overview:t num:1 reveal_rolling_links:t
#+OPTIONS: reveal_width:1200 reveal_height:800 toc:nil timestamp:nil reveal_mathjax:t
#+REVEAL_MARGIN: 0.05
#+REVEAL_MIN_SCALE: 0.5
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_TRANS: slide
#+REVEAL_SPEED: fast
#+REVEAL_THEME: solarized
#+REVEAL_HLEVEL: 1
#+REVEAL_EXTRA_CSS: ./extra_emnlp17.css

* Introduction

  #+BEGIN_NOTES
  - In this talk we present work on evaluating neurally generated text with respect to its capacity to preserve authorial style.
  #+END_NOTES

** RNNs are a powerful tool for text generation

   #+BEGIN_NOTES
   - Anecdotical: Famous pieces is the popular blog post by A. Karpathy and its many follow-ups.
   - Only recently have we started to see more thorough studies on the linguistic information encoded in RNNs
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   - Neurally generated text makes an authentic impression on readers
   - Basis for renewed interest in NLG
   - Recent interest in the information encoded by RNNs
   
   # #+reveal: split
   # #+BEGIN_NOTES
   # - RNNs avoid the Markov assumption that constrains NGLMs to modelling very local contexts
   # - RNNs seem to encode much more linguistic information
   # #+END_NOTES
   
   # #+attr_reveal: :frag (roll-in)
   # - Hypothetically, this is due to its ability to model (very) long-term dependencies...
   # - ...which allows it to mimic certain properties of the training data.

** Evaluation of neural synthetic text through Authorship Attribution

   #+BEGIN_NOTES
   For contrast, we wanted to study the potential of RNNs in comparison to traditional NGram models
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   - To what extent does the generative model retain author-related stylistic properties of the source?
   - How does it compare to traditional n-gram models?

** Experiment idea

   #+BEGIN_NOTES
   Very simple idea. Allows for many variations (evaluate different models, use different attributors)
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   - Fit Language Models (one per author)
   - Generate a collection of docs per author through sampling
   - Apply AA to the resulting synthetic dataset

*** Assumptions
    #+BEGIN_NOTES
    Controversial, but used as a symplifying assumption to get going.
    Of course, we could bias the algorithm towards more proper stylistic properties.
    #+END_NOTES
    #+attr_reveal: :frag (roll-in)
    "Algorithmic" definition of style
    #+attr_reveal: :frag (roll-in)
    #+BEGIN_QUOTE
    Style is whatever textual properties an algorithm might use in order to successfully 
    attribute a text to its actual author, without considering whether the
    properties discovered by the algorithm are stylistic in a narrower sense.
    #+END_QUOTE

* Summary of the presentation

  #+attr_reveal: :frag (roll-in)
  - Describe the models used for generation
  - Describe the experimental setup
  - Discuss the results

* Text Generation

  #+BEGIN_NOTES
  We approach the task of text generation with simple language models.
  We use character-level for simplicity and to tackle sparsity. However:
      - OOV during generation
      - Credit assignment paths span smaller linguistic sequences
  #+END_NOTES

** Character-level Text Generation with Language Models
   #+BEGIN_NOTES
   As a recap
   #+END_NOTES

   @@html:<span class="fragment highlight-green">@@        $P(w_1, w_2, ..., w_n)$ =
   @@html:</span><span class="fragment highlight-green">@@ $P(w_1|\text{<}bos\text{>})$
   @@html:</span><span class="fragment highlight-green">@@ $* \prod_{i=1}^n P(w_{i+1}|w_1, ..., w_{i})$
   @@html:</span>@@

*** Sample "n" characters from the Language Model
    #+attr_reveal: :frag (roll-in)
    - $w_1 \sim P(w|\text{<}bos\text{>})$
    - $w_2 \sim P(w|\text{<}bos\text{>}, w_1)$
    - $\ldots$
    - $w_n \sim P(w|\text{<}bos\text{>}, w_1, ..., w_{n-1})$

*** Multinomial sampling with temperature
    #+attr_reveal: :frag (roll-in)
    - $P(w|\text{<}bos\text{>}) = \{p_1, p_2, ..., p_v\}$
    - $p_i^{\tau} = \frac{p_i / \tau}{\sum_j^V p_j / \tau}$

** Models
*** NGLM
    #+BEGIN_NOTES
    Introduce Markov Assumption
    #+END_NOTES

    $P(w_t|\ldots)$ @@html:<span class="fragment fade-in">@@ $\approx P(w_t|w_{t-(n+1)}, \ldots, w_{t-1})$ @@html:</span>@@

*** RNNLM

    [[./img/rnnlm.svg]]

* Experiment

** Experimental Setup

*** Difficulties

    Maximize comparability of authentic and generated text
    #+attr_reveal: :frag (roll-in)
    - Unequal training size per author for LMs
    - Unequal training and test size per author for the attributor (important in AA)
    - Authentic text has doc-level structure, LM-generated text does not

*** Proposed method
    #+attr_reveal: :frag (roll-in)
    - Random even doc-level split (referred to as $\alpha$ and $\omega$ for simplicity)
    - Create 20 fixed-size (5000w) docs per split by sampling sentences
    - Sample a third set ($\bar{\alpha}$) from the author's LMs trained on $\alpha$

*** Attribution Experiments
    #+BEGIN_NOTES
    - Defines 5 experiments
    - Requires training only 3 classifiers (one per dataset)
    - Account for directionality effects
    #+END_NOTES

    #+BEGIN_EXPORT html
    <img src="./img/setup.svg">
    #+END_EXPORT

** Dataset

   #+BEGIN_NOTES
   - Most of the team members currently work on Latin
   - Interesting baseline since Ecclesiastical Latin was mostly L2
   - More space for developing authorial style (more interesting authorial patterns)
   - Tematically, realatively homogeneous
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   - Patrologia Latina (⩲ 113M words)
   - Ecclesiastical latin texts spanning 1000 years
   - Homogeneous institutionalized literary language (L2 language)
   
   #+reveal: split
   #+BEGIN_NOTES
   - We only use those authors (18) for which we had enough data for our experiment
   #+END_NOTES
   #+BEGIN_EXPORT html
   <img src="./img/author_words_docs.svg">
   #+END_EXPORT

** Language Model Fitting

*** NGLM
    #+attr_reveal: :frag (roll-in)
    - N-gram order is set to 6
    - Parameters estimated through MLE (no smoothing since only interested in generation)

*** RNNLM

    Model definition
    #+attr_reveal: :frag (roll-in)
    | Parameter      |   Value |
    |----------------+---------|
    | Embedding size |      24 |
    | RNN Cell       |    LSTM |
    | Hidden size    |     200 |
    | Hidden Layers  |       2 |

    #+reveal: split
    Training
    #+attr_reveal: :frag (roll-in)
    | Parameter              |                 Value |
    |------------------------+-----------------------|
    | Batch size             |                    50 |
    | Optimizer              | Adam (default params) |
    | Learning rate          |                 0.001 |
    | Gradient norm clipping |                   5.0 |
    | Dropout                |      0.3 (RNN output) |
    | Epochs                 |                    50 |

    #+reveal: split
    Validation perplexity@@html:: <span class="fragment highlight-green"><strong>4.015 (± 0.183)</strong></span>@@

** Attributor
   
   #+BEGIN_NOTES
   - max-features: ordered by term-freq
   - small grid, since computationally expensive (5-fold CV + 7 experiments)
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   - Linear SVM (scikit-learn) on tf-idf character {2,3,4}-grams
   - No word-level features since RNN can produce unseen words
   - Grid-search over parameters

   #+attr_reveal: :frag (roll-in)
   | Parameter    | Grid values                   |
   |--------------+-------------------------------|
   | Max-features | 5,000; 10,000; 15,000; 30,000 |
   | C            | 1; 10; 100; 1,000             |

* Results
  
  5-fold cross-validated scores per experiment

** Numbers

   #+BEGIN_NOTES
   - Very small directionality effect (confirm no artifacts from split)
   - Training on generated data: drops considerably (both NGLM and RNNLM)
   - Training on real data: improvement on NGLM, still drop for the RNN
   #+END_NOTES
  
   #+INCLUDE: "table.html" export html

** Discussion

*** Why does NGLM outperforms RNNLM in both setups ($<\bar{\alpha},\omega>, <\omega,\bar{\alpha}>$)?    

    #+attr_reveal: :frag (roll-in)
    SVM uses very local features, NGLM reproduces very well the local distribution

    #+BEGIN_NOTES
    Use local information (ngrams) to visualize the effect of local features on both models
    #+END_NOTES
  
    #+reveal: split
    #+BEGIN_NOTES
    - Compute a similarity matrix of generated and real authors on the basis of ngrams (2-4)
    - We use non-weighted Jaccard (intersection / union)
    - We expect a certain overlap for any author, and less overlap across authors
    #+END_NOTES
    LEXICAL OVERLAP: Mean-normalized ngram Jaccard-similarity across authors.
  
    @@html:<span class="fragment fade-in"><strong>Character-level</strong><br/><img src="./img/jaccard_mean_char.svg"></span>@@
  
    # #+reveal: split
    # #+BEGIN_NOTES
    # - Interestingly, we obtain a very similar picture at a higher-level (word-level)
    # #+END_NOTES
    # LEXICAL OVERLAP: Mean-normalized Jaccard-similarity on author ngrams
    
    # @@html:<span class=""><p><strong>Word-level</strong></p><img src="./img/jaccard_mean.svg"></span>@@

    #+reveal: split
    #+BEGIN_NOTES
    - For 3 authors, we take real and generated documents
    - We represent the documents using 150 most frequente char ngrams
    - Apply PCA to visualize the documents in space (for clustering)
    - We observe that this representation based on local features puts real and NGLM-generated text together
    #+END_NOTES
    DOC-LEVEL PCA: *NGLM* (150 most-freq ngrams doc-representation)
    #+BEGIN_EXPORT html
    <img src="./img/nglm_pca.svg">
    #+END_EXPORT
  
    #+reveal: split
    #+BEGIN_NOTES
    - Clustering based on local features separates real and RNNLM-generated in some cases
    - Reveals certain fuzzyness in the RNNLM-generated text
    #+END_NOTES
    DOC-LEVEL PCA: *RNNLM* (150 most-freq ngrams doc-representation)
    #+BEGIN_EXPORT html
    <img src="./img/rnnlm_pca.svg">
    #+END_EXPORT


*** Why does NGLM outperforms Real setup in ($\omega$, $\alpha$)?

    #+BEGIN_NOTES
    We haven't done any further analysis but judging on the basis of the previous viz
    #+END_NOTES
    #+attr_reveal: :frag (roll-in)
    - Prunning effect? Eliminating "distractive" features and enhancing those that are more relevant
    - It might prove beneficial for actual AA

   
* Self-learning (Data-augmentation) Experiments
  #+BEGIN_NOTES
  We were not satisfied, since RNNLM should have an advantage on modelling style, even though we only use very local feature to assess it.
  #+END_NOTES

  #+attr_reveal: :frag (roll-in)
  - Is there still some authorial signal in the RNNLM-generated data?
  - Is there an effect of the long-term dependencies learned by the RNNLM on the stylistic properties of the generated data?
  - If so, augmenting the authentic training data with RNNLM-generated data _could_ yield attribution improvements


** Experiment 
   #+BEGIN_NOTES
   For each author, concatenate authentic document collection with the generated one.
   #+END_NOTES
   
   #+attr_reveal: :frag (roll-in)
   $<\alpha+\bar{\alpha}, \omega>$

** Numbers
   
   #+BEGIN_NOTES
   - RNNLM increases (even over the <$\alpha, \omega$> baseline)
   - NGLM increases with respect to its baseline, but not over the real baseline
   #+END_NOTES

   # wrap to differentiate from other table
   #+BEGIN_EXPORT html
   <div id="table-full">
   #+END_EXPORT
   #+include: "table_full.html" export html
   #+BEGIN_EXPORT html
   </div>
   #+END_EXPORT

   #+reveal: split
   #+BEGIN_NOTES
   - Effect seems to be skewing the f1 distribution towards the top
   - Rescueing some helplessly mislabeled authors (helping the attribution of some guys)
   #+END_NOTES
   
   #+BEGIN_EXPORT html
   <img src="./img/f1_sampled.svg">
   #+END_EXPORT

** Discussion

   #+BEGIN_NOTES
   To summarize the argument
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   -> The long-term dependencies prove beneficial (not redundant)
   #+attr_reveal: :frag (roll-in)
   -> (Evidence for) RNNLM better modelling stylistic variation in the original distribution
   
* Conclusion

  #+BEGIN_NOTES
  Shortcomings:
  - Grid-search instead of random-search
  - Further fine-tuning of the networks (pretraining + overfitting?)
  - More compact models (CLMs (VAEs), multiheaded, ...)
  #+END_NOTES

  - LMs seem to capture stylistic properties to a certain extent
  - More global attributors still needed. Stylistic evaluation still too local
  - Unexpected result: data-augmentation/self-learning with RNNLMs possibly beneficial

* Thank you for your attention
