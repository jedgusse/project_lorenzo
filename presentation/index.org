# -*- org-reveal-title-slide: "<h1>%t</h1><h2>%a</h2><h4>%d</h4><h4>EMNLP17 (Workshop on Stylistic Variation) - Copenhagen</h4><p><a href=\"https://emanjavacas.github.com/slides-content/copenhagen-emnlp-17\">https://emanjavacas.github.com/slides-content/copenhagen-emnlp-17</a></p>"; -*-

#+TITLE: Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution
#+AUTHOR: Enrique Manjavacas & Jeroen De Gussem & Walter Daelemans & Mike Kestemont
#+DATE: 08/09/2017
# #+REVEAL_ROOT: ../externals/reveal.js/
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:t reveal_control:t
#+OPTIONS: reveal_keyboard:t reveal_overview:t num:1 reveal_rolling_links:t
#+OPTIONS: reveal_width:1200 reveal_height:800 toc:nil timestamp:nil reveal_mathjax:t
#+REVEAL_MARGIN: 0.05
#+REVEAL_MIN_SCALE: 0.5
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_TRANS: slide
#+REVEAL_SPEED: fast
#+REVEAL_THEME: solarized
#+REVEAL_HLEVEL: 1
#+REVEAL_EXTRA_CSS: ./extra_emnlp17.css

* Introduction

  #+BEGIN_NOTES
  We've been observed a resurgence in interest on Text Generation, allegedly due to the successes harvested by RNN models
  In this talk we present work on evaluating neurally generated text with respect to its 
  #+END_NOTES

** RNNs are a powerful tool for text generation

   #+BEGIN_NOTES
   Anecdotical: Famous pieces is the popular blog post by A. Karpathy and its many follow-ups.
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   - Neurally generated text makes an authentic impression on readers.
   - Basis for renewed interest in NLG.
   
   #+reveal: split
   #+BEGIN_NOTES
   - RNNs avoid the Markov assumption that constrains NGLMs to modelling very local contexts
   - RNNs seem to encode much more linguistic information
   #+END_NOTES
   
   #+attr_reveal: :frag (none roll-in)
   - Hypothetically, this is due to its ability to model (very) long-term dependencies...
   - ...which allows it to mimic certain properties of the training data.

** Evaluation of neural synthetic text through Authorship Attribution

   #+BEGIN_NOTES
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   - To what extent does the inferred generative distribution retain author-related stylistic properties of the empirical distribution?
   - How does it compare to traditional n-gram models?

** Experiment idea

   #+BEGIN_NOTES
   Very simple idea. Allows for many variations (evaluate different models, use different attributors)
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   - Fit LMs (one per author)
   - Generate a collection of docs per author through sampling
   - Apply AA to the resulting synthetic dataset

*** Assumptions
    #+BEGIN_NOTES
    Controversial, but used as a symplifying assumption to get going.
    Of course, we could bias the algorithm towards more proper stylistic properties.
    #+END_NOTES
    #+attr_reveal: :frag (roll-in)
    "Algorithmic" definition of style
    #+attr_reveal: :frag (roll-in)
    #+BEGIN_QUOTE
    Style is whatever textual properties an algorithm might use in order to successfully 
    attribute a text to its actual author, without considering whether the
    properties discovered by the algorithm are stylistic in a narrower sense.
    #+END_QUOTE

* Summary of the presentation

  #+attr_reveal: :frag (roll-in)
  - Describe the models used
  - Describe the actual experimental setup and associated issues
  - Discuss the results (with the help of some simple visualizations)
  - Conclusion

* Text Generation

  #+BEGIN_NOTES
  We approach the task of text generation with simple language models.
  We use character-level for simplicity and to tackle sparsity. However:
      - OOV during generation
      - Credit assignment paths span smaller linguistic sequences
  #+END_NOTES

** Character-level Text Generation with Language Models
   #+BEGIN_NOTES
   As a recap
   #+END_NOTES

   @@html:<span class="fragment highlight-green">@@        $P(w_1, w_2, ..., w_n)$ =
   @@html:</span><span class="fragment highlight-green">@@ $P(w_1|\text{<}bos\text{>})$
   @@html:</span><span class="fragment highlight-green">@@ $* \prod_{i=1}^n P(w_{i+1}|w_1, ..., w_{i})$
   @@html:</span>@@

*** Sample "n" characters from the Language Model
    #+attr_reveal: :frag (roll-in)
    - $w_1 \sim P(w|\text{<}bos\text{>})$
    - $w_2 \sim P(w|\text{<}bos\text{>}, w_1)$
    - $\ldots$
    - $w_n \sim P(w|\text{<}bos\text{>}, w_1, ..., w_{n-1})$

*** Multinomial sampling with temperature
    #+attr_reveal: :frag (roll-in)
    - $w_1 \sim P(w|\text{<}bos\text{>})$
    - $\Rightarrow w_1 = \{p_1, p_2, ..., p_v\}$
    - $p_i^{\tau} = \frac{p_i / \tau}{\sum_j^V p_j / \tau}$

** NGLM
   #+BEGIN_NOTES
   Introduce Markov Assumption
   #+END_NOTES

   $P(w_t|\ldots)$ @@html:<span class="fragment fade-in">@@ $\approx P(w_t|w_{t-(n+1)}, \ldots, w_{t-1})$ @@html:</span>@@

** RNNLM

   [[./img/rnnlm.svg]]


* Experiment

** Experimental Setup

*** Difficulties
    #+attr_reveal: :frag (roll-in)
    Maximize comparability of authentic and generated text
    #+attr_reveal: :frag (roll-in)
    - Unequal training size per author for LMs
    - Unequal training and test size per author for the attributor (important in AA)
    - Authentic text has doc-level structure, LM-generated text does not

*** Proposed method
    #+attr_reveal: :frag (roll-in)
    Procedure to create maximally-comparable semi-authentic sets
    #+attr_reveal: :frag (roll-in)
    - Random even doc-level split (50%-50%)
    - Create 20 fixed-size docs (5000 words) per split...  @@html:<span class="fragment">@@referred to as $\alpha$ and $\omega$ for simplicity@@html:</span>@@
    - For each author, sample a third set ($\bar{\alpha}$) from the author's LM

*** Attribution Experiments
    #+BEGIN_NOTES
    - Defines 5 experiments
    - Requires training only 3 classifiers (one per dataset)
    - Account for directionality effects
    #+END_NOTES

    #+BEGIN_EXPORT html
    <img src="./img/setup.svg">
    #+END_EXPORT

** Dataset

   #+BEGIN_NOTES
   We only used 18 * 100,000 * 2 (= 4M words)
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   - Patrologia Latina (⩲ 113M words)
   - Ecclesiastical latin texts spanning 1000 years
   - Homogeneous institutionalized literary language (L2 language)
   
   #+reveal: split
   #+BEGIN_EXPORT html
   <img src="./img/author_words_docs.svg">
   #+END_EXPORT

** Language Model Fitting

*** NGLM
    #+attr_reveal: :frag (roll-in)
    - N-gram order is set to 6
    - Parameters estimated through MLE (no smoothing since only interested in generation)

*** RNNLM

    Model definition
    #+attr_reveal: :frag (roll-in)
    | Parameter      |   Value |
    |----------------+---------|
    | Embedding size |      24 |
    | RNN Cell       |    LSTM |
    | Hidden size    |     200 |
    | Hidden Layers  |       2 |

    #+reveal: split
    Training
    #+attr_reveal: :frag (roll-in)
    | Parameter              |                 Value |
    |------------------------+-----------------------|
    | Batch size             |                    50 |
    | Optimizer              | Adam (default params) |
    | Learning rate          |                 0.001 |
    | Gradient norm clipping |                   5.0 |
    | Dropout                |      0.3 (RNN output) |
    | Epochs                 |                    50 |

    #+reveal: split
    Validation perplexity@@html:: <span class="fragment highlight-green"><strong>4.015 (± 0.183)</strong></span>@@

** Attributor
   
   #+BEGIN_NOTES
   - max-features: ordered by term-freq
   - small grid, since computationally expensive (5-fold CV + 7 experiments)
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   - Linear SVM (scikit-learn) on tf-idf character {2,3,4}-grams
   - No word-level features since RNN can produce unseen words
   - Grid-search over parameters

   #+attr_reveal: :frag (roll-in)
   | Parameter    | Grid values                   |
   |--------------+-------------------------------|
   | Max-features | 5,000; 10,000; 15,000; 30,000 |
   | C            | 1; 10; 100; 1,000             |

* Results
  
  5-fold cross-validated scores per experiment

** Numbers

   #+BEGIN_NOTES
   - Very small directionality effect (confirm no artifacts from split)
   - Training on generated data: drops considerably (both NGLM and RNNLM)
   - Training on real data: improvement on NGLM, still drop for the RNN
   #+END_NOTES
  
   #+INCLUDE: "table.html" export html

** Discussion

*** Why does NGLM outperforms RNNLM in both setups ($<\bar{\alpha},\omega>, <\omega,\bar{\alpha}>$)?    

    #+attr_reveal: :frag (roll-in)
    SVM uses very local features, NGLM reproduces very well the local distribution

    #+BEGIN_NOTES
    - Use local information (ngrams) to visualize the difference between the learned distribution and the real generative distribution (we only have indirect access to these distributions through sampling).
    #+END_NOTES
  
    #+reveal: split
    #+BEGIN_NOTES
    - clustering based on local features puts real and NGLM-generated text together
    #+END_NOTES
    DOC-LEVEL PCA: *NGLM* (150 most-freq ngrams doc-representation)
    #+BEGIN_EXPORT html
    <img src="./img/nglm_pca.svg">
    #+END_EXPORT
  
    #+reveal: split
    #+BEGIN_NOTES
    - clustering based on local features separates real and RNNLM-generated in some cases
    - Reveals certain fuzzyness in the RNNLM-generated text
    #+END_NOTES
    DOC-LEVEL PCA: *RNNLM* (150 most-freq ngrams doc-representation)
    #+BEGIN_EXPORT html
    <img src="./img/rnnlm_pca.svg">
    #+END_EXPORT
  
    #+reveal: split
    #+BEGIN_NOTES
    - jaccard: intersection / union
    - very strong overlap on the features in the case of NGLM
    #+END_NOTES
    LEXICAL OVERLAP@@html:<span class="fragment fade-in">: Mean-normalized ngram Jaccard-similarity across authors.</span>@@
  
    @@html:<span class="fragment fade-in"><strong>Character-level</strong><br/><img src="./img/jaccard_mean_char.svg"></span>@@
  
    #+reveal: split
    #+BEGIN_NOTES
    - Interestingly, we obtain a very similar picture at a higher-level (word-level)
    #+END_NOTES
    LEXICAL OVERLAP: Mean-normalized Jaccard-similarity on author ngrams
    
    @@html:<span class="fragment fade-in"><p><strong>Word-level</strong></p><img src="./img/jaccard_mean.svg"></span>@@

*** Why does NGLM outperforms Real setup in ($\omega$, $\alpha$)?

    #+BEGIN_NOTES
    We haven't done any further analysis but judging on the basis of the previous viz
    #+END_NOTES
    #+attr_reveal: :frag (roll-in)
    - Prunning effect? Eliminating "distractive" features and enhancing those that are more relevant
    - It might prove beneficial for actual AA
   
* Self-learning (Data-augmentation) Experiments
  #+BEGIN_NOTES
  We were not satisfied, since RNNLM should have an advantage on modelling style, even though we only use very local feature to assess it.
  #+END_NOTES

  #+attr_reveal: :frag (roll-in)
  - Is there some authorial signal in the RNNLM-generated data?
  - If so, augmenting the authentic training data with RNNLM-generated data _may_ yield attribution improvements

** Experiment 
   #+BEGIN_NOTES
   For each author, concatenate authentic document collection with the generated one.
   #+END_NOTES
   
   #+attr_reveal: :frag (roll-in)
   $<\alpha+\bar{\alpha}, \omega>$

** Numbers
   
   #+BEGIN_NOTES
   - RNNLM increases (even over the <$\alpha, \omega$> baseline)
   - NGLM increases with respect to its baseline, but not over the real baseline
   #+END_NOTES
   #+reveal: split
   # wrap to differentiate from other table
   #+BEGIN_EXPORT html
   <div id="table-full">
   #+END_EXPORT
   #+include: "table_full.html" export html
   #+BEGIN_EXPORT html
   </div>
   #+END_EXPORT

   #+reveal: split
   #+BEGIN_NOTES
   - Effect seems to be skewing the f1 distribution towards the top 
   - Rescueing some helplessly mislabeled authors (helping the attribution of some guys)
   #+END_NOTES
   
   #+BEGIN_EXPORT html
   <img src="./img/f1_sampled.svg">
   #+END_EXPORT

** Discussion

   #+BEGIN_NOTES
   To summarize the argument
   #+END_NOTES
   #+attr_reveal: :frag (roll-in)
   -> The long-term dependencies prove beneficial (not redundant).
   #+attr_reveal: :frag (roll-in)
   -> (Evidence for) generative distribution being closer to the original distribution.
   
* Conclusion

  #+BEGIN_NOTES
  Shortcomings:
  - Non-sequential attributor (very local context)
  - Grid-search instead of random-search
  - Further fine-tuning of the networks (pretraining + overfitting?)
  - More compact models (CLMs (VAEs), multiheaded, ...)
  #+END_NOTES

  - LMs seem to capture stylistic properties to a certain extent.
  - Although, stylistic evaluation still too local.
  - RNNLMs seem to exploit (although very data hungry).
  - Unexpected result: data-augmentation/self-learning with RNNLMs possibly beneficial.
